---
phase: 01-foundation
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - infra/foundry_config.json
  - infra/foundry_config.yaml
  - src/llm/client.py
  - src/llm/foundry_client.py
  - src/llm/__init__.py
  - scripts/validate_foundry_config.py
  - scripts/foundry_setup_smoke.py
autonomous: true

user_setup:
  - service: azure-ai-foundry
    why: "Real Foundry vision/text calls in Phase 02+ (mock mode works without it)"
    env_vars:
      - name: FOUNDRY_ENDPOINT
        source: "Azure AI Foundry / Azure OpenAI resource endpoint (e.g. https://<resource>.openai.azure.com)"
      - name: FOUNDRY_DEPLOYMENT
        source: "Azure AI Foundry deployment name for the model"
      - name: FOUNDRY_API_VERSION
        source: "Azure AI Foundry OpenAI API version (e.g. 2024-10-21)"
      - name: FOUNDRY_API_KEY
        source: "Azure portal / Foundry key (used as `api-key` header)"

must_haves:
  truths:
    - "The project has concrete Foundry config artifacts checked in (JSON canonical + YAML mirror)"
    - "LLM usage is behind a small client interface that can be mocked"
    - "With Foundry creds configured, a single real non-destructive request succeeds"
  artifacts:
    - path: "infra/foundry_config.json"
      provides: "Foundry project/config placeholders"
      contains: "project"
    - path: "infra/foundry_config.yaml"
      provides: "YAML mirror of Foundry config (for roadmap/tooling compatibility)"
      contains: "project"
    - path: "src/llm/client.py"
      provides: "LLM client interface used by agents"
      contains: "class LLMClient"
    - path: "src/llm/foundry_client.py"
      provides: "Foundry-backed LLM client implementation (config-only in Phase 1)"
      contains: "class FoundryLLMClient"
  key_links:
    - from: "src/llm/foundry_client.py"
      to: "infra/foundry_config.json"
      via: "load_config"
      pattern: "foundry_config"
---

<objective>
Create the Foundry config deliverable and a thin LLM client abstraction so later phases can use vision/text without hard-coding provider details.

Purpose: Avoid tangling core logic with provider-specific SDK calls; keep demo credible to Foundry constraints.
Output: `infra/foundry_config.json` + `src/llm/` interface.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@../../plans/stream-qa-swarm.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Foundry config artifact with strict, validated shape</name>
  <files>
 infra/foundry_config.json
 infra/foundry_config.yaml
 scripts/validate_foundry_config.py
 scripts/foundry_setup_smoke.py
  </files>
  <action>
 Create `infra/foundry_config.json` as the single source of truth for Foundry/LLM settings.

 - Include placeholders for: project name, model deployment name, endpoint base URL, and API version.
 - Also create `infra/foundry_config.yaml` as a YAML mirror of the same fields.
   - This YAML file is not the parsed source of truth in Phase 01; it's a mirror artifact to satisfy roadmap/tooling expectations.
  - Add `scripts/validate_foundry_config.py` that:
     - loads the JSON (stdlib `json`)
     - validates required keys exist
     - validates the YAML mirror file exists (do not add a YAML parser dependency)
     - prints a helpful error message + exits non-zero on missing fields.

  Create `scripts/foundry_setup_smoke.py` as the required "Foundry is alive" gate:
  - Validate `infra/foundry_config.json` via `scripts/validate_foundry_config.py`.
  - If required env vars are missing (see `user_setup`), print a copy/paste-able export block and exit non-zero.
  - If env vars are present, perform exactly ONE real, non-destructive HTTP request and exit non-zero on non-2xx.
    - Use a minimal chat-completions call with `max_tokens=1` to the Azure OpenAI-compatible endpoint:
      - URL: `${FOUNDRY_ENDPOINT}/openai/deployments/${FOUNDRY_DEPLOYMENT}/chat/completions?api-version=${FOUNDRY_API_VERSION}`
      - Header: `api-key: ${FOUNDRY_API_KEY}`
      - JSON body: `{ "messages": [{"role":"user","content":"ping"}], "max_tokens": 1 }`
      - Print only status code and a short success message (do not print response body if it might contain identifiers).
  </action>
  <verify>
  python3 scripts/validate_foundry_config.py --config infra/foundry_config.json
  </verify>
  <done>
Config files exist and the validator succeeds on the default template.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement LLMClient interface + Foundry implementation stub</name>
  <files>
src/llm/__init__.py
src/llm/client.py
src/llm/foundry_client.py
  </files>
  <action>
Create a minimal client interface used by the rest of the system.

- In `src/llm/client.py`, define `LLMClient` with methods intentionally small:
  - `vision_json(prompt, image_bytes, schema)` -> dict
  - `text(prompt)` -> str
 - In `src/llm/foundry_client.py`, implement `FoundryLLMClient` that:
   - loads config from `infra/foundry_config.json`
   - reads secrets from env vars (do not embed secrets in JSON)
   - raises a clear error if required env vars are missing
  - supports a mock fallback (`FOUNDRY_MOCK=1` or constructor flag) so development does not require credentials
  - may raise NotImplemented for actual network calls in Phase 1 (Phase 2 will implement real HTTP calls).
- Keep `src/llm/__init__.py` exporting the interface + default factory.
  </action>
  <verify>
python3 -m compileall src
python3 -c "from src.llm.client import LLMClient; from src.llm.foundry_client import FoundryLLMClient; print(LLMClient, FoundryLLMClient)"
  </verify>
  <done>
Agents can depend on `LLMClient` without importing provider SDKs directly.
  </done>
</task>

<!-- Live Foundry smoke is intentionally non-blocking.
     If/when credentials are available, run:
     `python3 scripts/foundry_setup_smoke.py --config infra/foundry_config.json`
-->

</tasks>

<verification>
- Config validator is runnable and fails loudly on missing keys.
- LLM client import surface is stable (`src/llm/*`).
- `scripts/foundry_setup_smoke.py` blocks without credentials and succeeds with a single real request once env vars are set.
</verification>

<success_criteria>
 - FOUND-01 unblocked: Foundry configuration artifacts exist, required creds are enumerated, and live connectivity can be validated via a single non-destructive request.
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
